{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24291636",
   "metadata": {
    "id": "24291636"
   },
   "source": [
    "# Melanoma, Fundoscopy - ResNet50\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13fdfcf",
   "metadata": {
    "id": "e13fdfcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to run the training process? (y / n)\n",
      "y\n",
      "File path is /home/arttu/Documents/research/melanoma/\n",
      "Device is cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(\"Do you want to run the training process? (y / n)\")\n",
    "i = input()\n",
    "if i == 'y':\n",
    "    train_model = True\n",
    "else:\n",
    "    train_model = False\n",
    "    \n",
    "fpath = \"/home/arttu/Documents/research/melanoma/\"\n",
    "# fpath = \"hakkina7@triton.aalto.fi:/scratch/work/hakkina7/research/melanoma/\"\n",
    "print(\"File path is {}\".format(fpath))\n",
    "\n",
    "# Set the device\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"Device is {}\".format(device))\n",
    "\n",
    "# Set file name\n",
    "fname = 'resnet50.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bbaeb",
   "metadata": {
    "id": "a51bbaeb"
   },
   "source": [
    "## Data preprocessing\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d3e7c4",
   "metadata": {
    "id": "85d3e7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some images? (y / n)\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Takes a while to run the mean and std calculation, so save the values here\n",
    "# NB: These have to be calculated again if the image size is changed\n",
    "# NB: Modifying the batch size doesn't matter, mean and std stay the same\n",
    "# mean = [0.6678, 0.5298, 0.5245] # h, w = (224, 224)\n",
    "# std = [0.1331, 0.1473, 0.1587] # h, w = (224, 224)\n",
    "\n",
    "h, w = (224, 224)\n",
    "# Get proper dataloader with normalized images\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((h, w)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor([0.0, 0.0, 0.0]), torch.Tensor([1.0, 1.0, 1.0]))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=fpath+\"data/train/\",\n",
    "                                           transform=transformations)\n",
    "testset = torchvision.datasets.ImageFolder(root=fpath+\"data/test/\",\n",
    "                                           transform=transformations)\n",
    "valset = torchvision.datasets.ImageFolder(root=fpath+\"data/val/\",\n",
    "                                           transform=transformations)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=8, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Show some images? (y / n)\")\n",
    "i = input()\n",
    "if i == 'y':\n",
    "    show = True\n",
    "else:\n",
    "    show = False\n",
    "\n",
    "if show:\n",
    "    # Show some images after transformations\n",
    "    loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=9, shuffle=True)\n",
    "    images, labels = next(iter(loader))\n",
    "    grid=torchvision.utils.make_grid(images, nrow=3)\n",
    "    plt.figure(figsize=(10,40))\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf30948",
   "metadata": {
    "id": "bbf30948"
   },
   "source": [
    "## Training\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069ba962",
   "metadata": {
    "id": "069ba962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 model for Fundoscopy classification task.\n",
      "Since there is class imbalance present in some of the data sets, use the following weights in the loss function:\n",
      "tensor([ 29.1988,   7.6238,   9.6541, 106.0942,   5.6009,   1.9674,  40.3665,\n",
      "        100.3168])\n",
      "\n",
      "Batch 1/634: 11.02 seconds\n",
      "Batch 2/634: 10.56 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-768b1d5aabe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m                                       momentum=0.9),\n\u001b[1;32m    123\u001b[0m                   \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                   weights=weights)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-768b1d5aabe6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, trainloader, testloader, device, optimizer, loss_fn, weights)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/leanheat/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, epochs, trainloader, testloader, device, optimizer, loss_fn, weights):\n",
    "    \"\"\"Training loop\"\"\"\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        best_acc = 0\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start = time.time()\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            correct += (labels == pred).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Batch {}/{}: {:.2f} seconds\".format(i+1, len(trainloader), time.time()-start))\n",
    "            start=time.time()\n",
    "        acc = 100 * correct/total\n",
    "        print(\"\\nEpoch {}:\\nMean loss {} - Correctly labeled training images {:.2f}%\".format(epoch+1,\n",
    "                                                                                           np.mean(losses),\n",
    "                                                                                           acc))\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        true = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "        preds = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "        probs = torch.zeros(0, dtype=torch.float, device='cpu')\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            for i, (images, labels) in enumerate(testloader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, pred = torch.max(outputs, 1)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                true = torch.cat([true, labels.view(-1)])\n",
    "                preds = torch.cat([preds, pred.view(-1)])\n",
    "                prob, _ = (F.softmax(outputs, dim=1)).topk(1, dim=1)\n",
    "                probs = torch.cat([probs, prob.view(-1)])\n",
    "                print(\"Batch {}/{}: {:.2f} seconds\".format(i+1, len(testloader), time.time()-start))\n",
    "                start=time.time()\n",
    "                \n",
    "        acc = 100 * correct/total\n",
    "        print(\"Classified {:.2f} % of test images correctly.\".format(\n",
    "            acc\n",
    "        ))\n",
    "        print(\"Precision on test set: {}\".format(\n",
    "            metrics.precision_score(true.numpy(),\n",
    "                                    preds.numpy(),\n",
    "                                    average=None)\n",
    "        ))\n",
    "        print(\"Recall on test set: {}\".format(\n",
    "            metrics.recall_score(true.numpy(),\n",
    "                                 preds.numpy(), \n",
    "                                 average=None)\n",
    "        ))\n",
    "        print(\"F1-score on test set: {}\".format(\n",
    "            metrics.f1_score(true.numpy(),\n",
    "                             preds.numpy(),\n",
    "                             average=None)\n",
    "        ))\n",
    "        classes = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
    "        aucs = {}\n",
    "        for i, c in enumerate(classes):\n",
    "            fpr, tpr, _ = metrics.roc_curve(true.numpy(),\n",
    "                                            probs.numpy(), \n",
    "                                            pos_label=i)\n",
    "            aucs[c] = metrics.auc(fpr, tpr)\n",
    "        print(\"ROC AUC on test set: {}\".format(aucs))\n",
    "        print(\"Confusion matrix on test set:\")\n",
    "        print(pd.DataFrame(metrics.confusion_matrix(true.numpy(), preds.numpy()), \n",
    "                           columns=['pred as AK', 'pred as BCC', \n",
    "                                    'pred as BKL', 'pred as DF',\n",
    "                                    'pred as MEL', 'pred as NV',\n",
    "                                    'pred as SCC', 'pred as VASC'], \n",
    "                           index=['true class AK', 'true class BCC',\n",
    "                                  'true class BKL', 'true class DF',\n",
    "                                  'true class MEL', 'true class NV',\n",
    "                                  'true class SCC', 'true class VASC']))\n",
    "        print()\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if train_model:\n",
    "    print(\"Training ResNet50 model for Fundoscopy classification task.\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    n_labels = 8\n",
    "    n_inputs = model.fc.in_features\n",
    "    model.fc = nn.Linear(n_inputs, n_labels)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Calculate weights for the weighted loss function. We need the weighted loss,\n",
    "    # because of the class imbalance that is present on the data sets. If\n",
    "    # weighted loss fails, try focal loss with alpha=0.25 and gamma=2.\n",
    "    n_samples = [694, 2658, 2099, 191, 3618, 10300, 502, 202]\n",
    "    weights = [1 / (x / sum(n_samples)) for x in n_samples]  # More weight on the samples that appear less\n",
    "    weights = torch.FloatTensor(weights).to(device)\n",
    "    print(\"Since there is class imbalance present in some of the data sets, use the following weights in the loss function:\")\n",
    "    print(weights)\n",
    "    print()\n",
    "    \n",
    "    # Train model\n",
    "    model = train(model=model,\n",
    "                  epochs=100,\n",
    "                  trainloader=trainloader,\n",
    "                  testloader=testloader,\n",
    "                  device=device,\n",
    "                  optimizer=optim.SGD(list(model.parameters()),\n",
    "                                      lr=1e-3,\n",
    "                                      momentum=0.9),\n",
    "                  loss_fn=nn.CrossEntropyLoss(weight=weights),\n",
    "                  weights=weights)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), fpath+fname)\n",
    "    print(\"Model succesfully saved to {}\".format(fpath+fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7bc64",
   "metadata": {
    "id": "31d7bc64"
   },
   "source": [
    "## Evaluate\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494753c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 139507,
     "status": "error",
     "timestamp": 1634879482892,
     "user": {
      "displayName": "Arttu HÃ¤kkinen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02318314570735220246"
     },
     "user_tz": -180
    },
    "id": "494753c7",
    "outputId": "698e6933-dd1b-403a-e30d-ff993b88c6d5"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valloader, device):\n",
    "    \"\"\"Evaluate model performance on validation set\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "    preds = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "    probs = torch.zeros(0, dtype=torch.float, device='cpu')\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for i, (images, labels) in enumerate(valloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            true = torch.cat([true, labels.view(-1)])\n",
    "            preds = torch.cat([preds, pred.view(-1)])\n",
    "            prob, _ = (F.softmax(outputs, dim=1)).topk(1, dim=1)\n",
    "            probs = torch.cat([probs, prob.view(-1)])\n",
    "            print(\"Batch {}/{}: {:.2f} seconds\".format(i+1, len(valloader), time.time()-start))\n",
    "            start=time.time()\n",
    "    \n",
    "    acc = 100 * correct/total\n",
    "    print(\"Classified {:.2f} % of validation images correctly.\".format(\n",
    "        acc\n",
    "    ))\n",
    "    print(\"Precision on validation set: {}\".format(\n",
    "        metrics.precision_score(true.numpy(),\n",
    "                                preds.numpy(),\n",
    "                                average=None)\n",
    "    ))\n",
    "    print(\"Recall on validation set: {}\".format(\n",
    "        metrics.recall_score(true.numpy(),\n",
    "                             preds.numpy(), \n",
    "                             average=None)\n",
    "    ))\n",
    "    print(\"F1-score on validation set: {}\".format(\n",
    "        metrics.f1_score(true.numpy(),\n",
    "                         preds.numpy(),\n",
    "                         average=None)\n",
    "    ))\n",
    "    classes = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
    "    aucs = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        fpr, tpr, _ = metrics.roc_curve(true.numpy(),\n",
    "                                        probs.numpy(), \n",
    "                                        pos_label=i)\n",
    "        aucs[c] = metrics.auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label='{} AUC = {:.2f}'.format(c, metrics.auc(fpr, tpr)))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    print(\"ROC AUC on validation set: {}\".format(aucs))\n",
    "    print(\"Confusion matrix on validation set:\")\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(true.numpy(), preds.numpy()), \n",
    "                       columns=['pred as AK', 'pred as BCC', \n",
    "                                'pred as BKL', 'pred as DF',\n",
    "                                'pred as MEL', 'pred as NV',\n",
    "                                'pred as SCC', 'pred as VASC'], \n",
    "                       index=['true class AK', 'true class BCC',\n",
    "                              'true class BKL', 'true class DF',\n",
    "                              'true class MEL', 'true class NV',\n",
    "                              'true class SCC', 'true class VASC']))\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Initialize model\n",
    "model = models.inception_v3(pretrained=False)\n",
    "n_labels = 8\n",
    "n_inputs = model.fc.in_features\n",
    "model.fc = nn.Linear(n_inputs, n_labels)\n",
    "\n",
    "# Load model params\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(fpath+fname))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(fpath+fname,\n",
    "                                     map_location=torch.device('cpu')))\n",
    "    \n",
    "print(\"Model succesfully loaded from {}\".format(fpath+fname))\n",
    "\n",
    "# Evaluate the model performance on validation set\n",
    "print(\"Evaluating the ResNet50 model on the validation set.\")\n",
    "evaluate(model, valloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a73eb",
   "metadata": {
    "id": "ee0a73eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DR_RESNET50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
